{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6d8b88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation complete for: data, processed, cmip6\n",
      "Summary: 22/23 models complete. Parsed log (up to 'grid') written to /mnt/team/rapidresponse/pub/tropical-storms/data/processed/cmip6/validation_log.json\n",
      "\n",
      "Validation complete for: tc_risk, input, cmip6\n",
      "Summary: 22/23 models complete. Parsed log (up to 'scenario') written to /mnt/team/rapidresponse/pub/tropical-storms/tc_risk/input/cmip6/validation_log.json\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import sys\n",
    "import getpass\n",
    "import uuid\n",
    "import os\n",
    "from jobmon.client.tool import Tool \n",
    "from pathlib import Path\n",
    "\n",
    "# NOTE: These imports rely on external module definitions (constants, io_compare_utils, etc.)\n",
    "import idd_climate_models.constants as rfc\n",
    "from idd_climate_models.io_compare_utils import compare_model_validation\n",
    "from idd_climate_models.dictionary_utils import parse_results\n",
    "from idd_climate_models.resource_functions import get_rep_file_size_gb, get_resource_tier\n",
    "\n",
    "# --- CONSTANT DEFINITIONS (from rfc) ---\n",
    "repo_name = rfc.repo_name\n",
    "package_name = rfc.package_name\n",
    "DATA_DIR = rfc.RAW_DATA_PATH\n",
    "PROCESSED_DATA_PATH = rfc.PROCESSED_DATA_PATH\n",
    "TC_RISK_INPUT_PATH = rfc.TC_RISK_INPUT_PATH\n",
    "SCRIPT_ROOT = rfc.REPO_ROOT / repo_name / \"src\" / package_name / \"01_run_tc_risk\"\n",
    "\n",
    "# Configuration\n",
    "DATA_SOURCE = \"cmip6\"\n",
    "BIN_SIZE_YEARS = 20\n",
    "DRY_RUN = False # Assuming DRY_RUN is generally False for submission\n",
    "RERUN = False\n",
    "\n",
    "\n",
    "INPUT_DATA_TYPE = \"data\"\n",
    "INPUT_IO_TYPE = \"processed\"\n",
    "OUTPUT_DATA_TYPE = \"tc_risk\"\n",
    "OUTPUT_IO_TYPE = \"input\"\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# DATA SETUP & VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "# Use the unified function for validation and comparison\n",
    "validation_info = compare_model_validation(\n",
    "    input_data_type=INPUT_DATA_TYPE,\n",
    "    input_io_type=INPUT_IO_TYPE,\n",
    "    output_data_type=OUTPUT_DATA_TYPE,\n",
    "    output_io_type=OUTPUT_IO_TYPE,\n",
    "    data_source=DATA_SOURCE,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "models_to_process = validation_info[\"models_to_process\"]\n",
    "model_variants_to_run = parse_results(validation_info[\"models_to_process_dict\"], 'variant')\n",
    "\n",
    "# Get the full hierarchy list to build the variable detail map\n",
    "full_path_list = parse_results(validation_info[\"models_to_process_dict\"], 'all')\n",
    "variable_detail_map = {}\n",
    "\n",
    "# Build the map: {(model, variant, scenario, variable): {'grid': 'gn', 'frequency': 'day'}}\n",
    "for item in full_path_list:\n",
    "    key = (\n",
    "        item['model'],\n",
    "        item['variant'],\n",
    "        item['scenario'],\n",
    "        item['variable']\n",
    "    )\n",
    "    # Store the unique grid and frequency needed to build the source_dir\n",
    "    variable_detail_map[key] = {\n",
    "        'grid': item['grid'],\n",
    "        'frequency': item['frequency']\n",
    "    }\n",
    "\n",
    "def get_time_bins(scenario_name, bin_size_years):\n",
    "    date_ranges = rfc.VALIDATION_RULES['tc_risk']['time-period']['date_ranges']\n",
    "    if scenario_name not in date_ranges:\n",
    "        print(f\"Warning: No date range found for scenario '{scenario_name}'\")\n",
    "        return []\n",
    "    start_year, end_year = date_ranges[scenario_name]\n",
    "    return [(y, min(y + bin_size_years - 1, end_year)) for y in range(start_year, end_year + 1, bin_size_years)]\n",
    "\n",
    "TIME_BINS = {\n",
    "    scenario: get_time_bins(scenario, BIN_SIZE_YEARS)\n",
    "    for scenario in rfc.SCENARIOS\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7ba6dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ANALYZING FAILURES FOR INCORRECTLY FILTERED MODELS\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "ANALYZING FAILURES (Highest-Level Issue Per Incomplete Model)\n",
      "================================================================================\n",
      "✗ INCOMPLETE: Validation Failed at: **Model=AWI-CM-1-1-MR -> variant=r1i1p1f1 -> scenario=historical -> variable=tos -> grid=gn -> frequency=Omon**\n",
      "Issues (1): Forbidden unstructured grid dimensions found: ['ncells']. Model is incompatible with the target TC risk grid.\n",
      "================================================================================\n",
      "Model: AWI-CM-1-1-MR -> ✗ INCOMPLETE: Validation Failed at: **Model=AWI-CM-1-1-MR -> variant=r1i1p1f1 -> scenario=historical -> variable=tos -> grid=gn -> frequency=Omon**\n",
      "Issues (1): Forbidden unstructured grid dimensions found: ['ncells']. Model is incompatible with the target TC risk grid.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Assuming validation_info exists from the pipeline run\n",
    "from idd_climate_models.dictionary_utils import summarize_all_failures\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ANALYZING FAILURES FOR INCORRECTLY FILTERED MODELS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Summarize the failures from the input validation dictionary\n",
    "failure_summary = summarize_all_failures(validation_info['input_validation_dict'])\n",
    "\n",
    "# Print the specific issues\n",
    "for model, summary in failure_summary.items():\n",
    "    print(f\"Model: {model} -> {summary}\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "663f00b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'complete': False,\n",
       " 'files': [{'path': '/mnt/team/rapidresponse/pub/tropical-storms/tc_risk/input/cmip6/AWI-CM-1-1-MR/r1i1p1f1/ssp245/2095-2100/hus_Amon_AWI-CM-1-1-MR_ssp245_r1i1p1f1_gn_209501-210012.nc'},\n",
       "  {'path': '/mnt/team/rapidresponse/pub/tropical-storms/tc_risk/input/cmip6/AWI-CM-1-1-MR/r1i1p1f1/ssp245/2095-2100/psl_Amon_AWI-CM-1-1-MR_ssp245_r1i1p1f1_gn_209501-210012.nc'},\n",
       "  {'path': '/mnt/team/rapidresponse/pub/tropical-storms/tc_risk/input/cmip6/AWI-CM-1-1-MR/r1i1p1f1/ssp245/2095-2100/ta_Amon_AWI-CM-1-1-MR_ssp245_r1i1p1f1_gn_209501-210012.nc'},\n",
       "  {'path': '/mnt/team/rapidresponse/pub/tropical-storms/tc_risk/input/cmip6/AWI-CM-1-1-MR/r1i1p1f1/ssp245/2095-2100/ua_day_AWI-CM-1-1-MR_ssp245_r1i1p1f1_gn_20950101-21001231.nc'},\n",
       "  {'path': '/mnt/team/rapidresponse/pub/tropical-storms/tc_risk/input/cmip6/AWI-CM-1-1-MR/r1i1p1f1/ssp245/2095-2100/va_day_AWI-CM-1-1-MR_ssp245_r1i1p1f1_gn_20950101-21001231.nc'}],\n",
       " 'issues': [\"Missing required variable files: ['tos']\"]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_info['output_validation_dict']['validation_results']['AWI-CM-1-1-MR']['variant']['r1i1p1f1']['scenario']['ssp245']['time-period']['2095-2100']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "681b8409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files found in source: 54\n",
      "Files selected for 5-year bin (tos Omon): 5\n",
      "Single file size: 0.158 GiB\n"
     ]
    }
   ],
   "source": [
    "# CELL 1: Setup and Configuration (Modified to select 5 files)\n",
    "import xarray as xr\n",
    "import os\n",
    "from pathlib import Path\n",
    "from memory_profiler import memory_usage\n",
    "import time\n",
    "\n",
    "# --- Path to the Directory Containing the Yearly Files ---\n",
    "FILE_PATH = Path(\"/mnt/team/rapidresponse/pub/tropical-storms/data/processed/cmip6/AWI-CM-1-1-MR/r1i1p1f1/historical/tos/gn/Omon\")\n",
    "OUTPUT_FILE = Path(\"./temp_tos_combined_test.nc\")\n",
    "\n",
    "# We select the first 5 yearly files for the test (simulating a 5-year bin)\n",
    "file_list = sorted([f for f in os.listdir(FILE_PATH) if f.endswith('.nc')])\n",
    "files_to_process = file_list[:5] # <-- Changed to 5 files (1 per year)\n",
    "\n",
    "print(f\"Total files found in source: {len(file_list)}\")\n",
    "print(f\"Files selected for 5-year bin (tos Omon): {len(files_to_process)}\")\n",
    "\n",
    "# Test the size of a single file (should be ~169MB)\n",
    "if files_to_process:\n",
    "    single_file_size_gb = os.path.getsize(FILE_PATH / files_to_process[0]) / (1024**3)\n",
    "    print(f\"Single file size: {single_file_size_gb:.3f} GiB\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"No files found to process.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efd96542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting file: tos_Omon_ACCESS-CM2_historical_r1i1p1f1_gn_195001-195012.nc\n",
      "\n",
      "--- Dataset Dimensions & Variables for ACCESS-CM2 ---\n",
      "<xarray.Dataset> Size: 14MB\n",
      "Dimensions:             (time: 12, bnds: 2, j: 300, i: 360, vertices: 4)\n",
      "Coordinates:\n",
      "  * time                (time) datetime64[ns] 96B 1950-01-16T12:00:00 ... 195...\n",
      "  * j                   (j) int32 1kB 0 1 2 3 4 5 6 ... 294 295 296 297 298 299\n",
      "  * i                   (i) int32 1kB 0 1 2 3 4 5 6 ... 354 355 356 357 358 359\n",
      "    latitude            (j, i) float64 864kB ...\n",
      "    longitude           (j, i) float64 864kB ...\n",
      "Dimensions without coordinates: bnds, vertices\n",
      "Data variables:\n",
      "    time_bnds           (time, bnds) datetime64[ns] 192B ...\n",
      "    vertices_latitude   (j, i, vertices) float64 3MB ...\n",
      "    vertices_longitude  (j, i, vertices) float64 3MB ...\n",
      "    tos                 (time, j, i) float32 5MB ...\n",
      "Attributes: (12/47)\n",
      "    Conventions:            CF-1.7 CMIP-6.2\n",
      "    activity_id:            CMIP\n",
      "    branch_method:          standard\n",
      "    branch_time_in_child:   0.0\n",
      "    branch_time_in_parent:  0.0\n",
      "    creation_date:          2019-11-08T18:45:44Z\n",
      "    ...                     ...\n",
      "    variable_id:            tos\n",
      "    variant_label:          r1i1p1f1\n",
      "    version:                v20191108\n",
      "    cmor_version:           3.4.0\n",
      "    tracking_id:            hdl:21.14100/0bcaaa74-aedb-4d45-a5e5-cb3ab467f2b5\n",
      "    license:                CMIP6 model data produced by CSIRO is licensed un...\n",
      "\n",
      "--- Coordinates and Their Shapes ---\n",
      "Coordinate: time, Shape: (12,), Dims: ('time',)\n",
      "Coordinate: j, Shape: (300,), Dims: ('j',)\n",
      "Coordinate: i, Shape: (360,), Dims: ('i',)\n",
      "Coordinate: latitude, Shape: (300, 360), Dims: ('j', 'i')\n",
      "Coordinate: longitude, Shape: (300, 360), Dims: ('j', 'i')\n",
      "\n",
      "--- Check Memory Estimate of Single File ---\n",
      "Total estimated memory (raw data): 0.013 GiB\n"
     ]
    }
   ],
   "source": [
    "# CELL 1: Load a Single File and Inspect Metadata\n",
    "\n",
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# --- Path to the ACCESS-CM2 Directory ---\n",
    "FILE_PATH = Path(\"/mnt/team/rapidresponse/pub/tropical-storms/data/processed/cmip6/ACCESS-CM2/r1i1p1f1/historical/tos/gn/Omon\")\n",
    "file_list = sorted([f for f in os.listdir(FILE_PATH) if f.endswith('.nc')])\n",
    "first_file = FILE_PATH / file_list[0]\n",
    "\n",
    "print(f\"Inspecting file: {first_file.name}\")\n",
    "\n",
    "# 1. Open a single dataset lazily\n",
    "ds = xr.open_dataset(first_file)\n",
    "\n",
    "print(f\"\\n--- Dataset Dimensions & Variables for ACCESS-CM2 ---\")\n",
    "print(ds)\n",
    "\n",
    "print(f\"\\n--- Coordinates and Their Shapes ---\")\n",
    "# Check the shape of the lat/lon arrays. If they are simple 1D arrays, memory overhead is low.\n",
    "for name, data_array in ds.coords.items():\n",
    "    print(f\"Coordinate: {name}, Shape: {data_array.shape}, Dims: {data_array.dims}\")\n",
    "\n",
    "print(f\"\\n--- Check Memory Estimate of Single File ---\")\n",
    "print(f\"Total estimated memory (raw data): {ds.nbytes / (1024**3):.3f} GiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e598f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Dataset Dimensions & Variables ---\n",
      "<xarray.Dataset> Size: 624MB\n",
      "Dimensions:    (time: 120, bnds: 2, ncells: 830305, vertices: 16)\n",
      "Coordinates:\n",
      "  * time       (time) datetime64[ns] 960B 1961-01-16T12:00:00 ... 1970-12-16T...\n",
      "    lat        (ncells) float64 7MB ...\n",
      "    lon        (ncells) float64 7MB ...\n",
      "Dimensions without coordinates: bnds, ncells, vertices\n",
      "Data variables:\n",
      "    time_bnds  (time, bnds) datetime64[ns] 2kB ...\n",
      "    tos        (time, ncells) float32 399MB ...\n",
      "    lat_bnds   (ncells, vertices) float64 106MB ...\n",
      "    lon_bnds   (ncells, vertices) float64 106MB ...\n",
      "Attributes: (12/39)\n",
      "    frequency:              mon\n",
      "    activity_id:            CMIP\n",
      "    Conventions:            CF-1.7 CMIP-6.2\n",
      "    creation_date:          2018-12-18T12:00:00Z\n",
      "    data_specs_version:     01.00.27\n",
      "    experiment:             historical\n",
      "    ...                     ...\n",
      "    parent_activity_id:     CMIP\n",
      "    parent_experiment_id:   piControl\n",
      "    parent_mip_era:         CMIP6\n",
      "    parent_source_id:       AWI-CM-1-1-MR\n",
      "    parent_time_units:      days since 2401-1-1\n",
      "    parent_variant_label:   r1i1p1f1\n",
      "\n",
      "--- Coordinates and Their Shapes ---\n",
      "Coordinate: time, Shape: (120,), Dims: ('time',)\n",
      "Coordinate: lat, Shape: (830305,), Dims: ('ncells',)\n",
      "Coordinate: lon, Shape: (830305,), Dims: ('ncells',)\n",
      "\n",
      "--- Check Memory Estimate of Single File ---\n",
      "Total estimated memory (raw data): 0.582 GiB\n"
     ]
    }
   ],
   "source": [
    "# CELL 1: Load a Single File and Inspect Metadata\n",
    "\n",
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "\n",
    "FILE_PATH = Path(\"/mnt/team/rapidresponse/pub/tropical-storms/data/raw/cmip6/AWI-CM-1-1-MR/r1i1p1f1/historical/tos/gn/Omon\")\n",
    "file_list = sorted([f for f in os.listdir(FILE_PATH) if f.endswith('.nc')])\n",
    "first_file = FILE_PATH / file_list[0]\n",
    "\n",
    "# 1. Open a single dataset lazily\n",
    "ds = xr.open_dataset(first_file)\n",
    "\n",
    "print(f\"--- Dataset Dimensions & Variables ---\")\n",
    "print(ds)\n",
    "\n",
    "print(f\"\\n--- Coordinates and Their Shapes ---\")\n",
    "# Identify any coordinates or variables that have a large number of indices or dimensions.\n",
    "# Look for large arrays that are not the main 'tos' data variable.\n",
    "for name, data_array in ds.coords.items():\n",
    "    print(f\"Coordinate: {name}, Shape: {data_array.shape}, Dims: {data_array.dims}\")\n",
    "\n",
    "print(f\"\\n--- Check Memory Estimate of Single File ---\")\n",
    "# This estimates the memory of the actual data, NOT the overhead.\n",
    "# Look for a large discrepancy between the .nbytes estimate and the file size (0.165 GiB).\n",
    "print(f\"Total estimated memory (raw data): {ds.nbytes / (1024**3):.3f} GiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25115344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of grid cells (ncells): 830305\n",
      "Shape of latitude array: (830305,)\n",
      "Shape of longitude array: (830305,)\n",
      "Number of unique latitudes: 830305\n",
      "Number of unique longitudes: 830305\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "FILE_PATH = Path(\"/mnt/team/rapidresponse/pub/tropical-storms/data/processed/cmip6/AWI-CM-1-1-MR/r1i1p1f1/historical/tos/gn/Omon\")\n",
    "file_list = sorted([f for f in os.listdir(FILE_PATH) if f.endswith('.nc')])\n",
    "first_file = FILE_PATH / file_list[0]\n",
    "\n",
    "# 1. Open the dataset lazily\n",
    "ds_awi = xr.open_dataset(first_file)\n",
    "\n",
    "# 2. Extract the coordinate arrays (these are large 1D arrays of size 830,305)\n",
    "awi_latitudes = ds_awi['lat'].values\n",
    "awi_longitudes = ds_awi['lon'].values\n",
    "\n",
    "print(f\"Total number of grid cells (ncells): {len(awi_latitudes)}\")\n",
    "print(f\"Shape of latitude array: {awi_latitudes.shape}\")\n",
    "print(f\"Shape of longitude array: {awi_longitudes.shape}\")\n",
    "print(f\"Number of unique latitudes: {len(set(awi_latitudes))}\")\n",
    "print(f\"Number of unique longitudes: {len(set(awi_longitudes))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79f160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can I plot the lat/lon arrays to visualize their distribution?\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(awi_longitudes, awi_latitudes, s=1)\n",
    "plt.title(\"Scatter Plot of AWI-CM-1-1-MR Grid Points\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e878e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np # Used for finding global min/max\n",
    "\n",
    "# --- Configuration ---\n",
    "# Use the raw data path to get a full file for analysis\n",
    "FILE_PATH = Path(\"/mnt/team/rapidresponse/pub/tropical-storms/data/raw/cmip6/AWI-CM-1-1-MR/r1i1p1f1/historical/tos/gn/Omon\")\n",
    "\n",
    "# --- Processing ---\n",
    "\n",
    "try:\n",
    "    file_list = sorted([f for f in os.listdir(FILE_PATH) if f.endswith('.nc')])\n",
    "    if not file_list:\n",
    "        print(f\"Error: No NetCDF files found in {FILE_PATH}\")\n",
    "        exit()\n",
    "        \n",
    "    first_file = FILE_PATH / file_list[0]\n",
    "    \n",
    "    # Open the dataset, loading the first 12 months (or fewer if the file is shorter)\n",
    "    with xr.open_dataset(first_file) as ds_awi:\n",
    "        # Select the first 12 months for plotting (the first year)\n",
    "        # We must compute() here to pull the coordinates and tos data into memory for plotting\n",
    "        ds_12_months = ds_awi.isel(time=slice(0, 12)).compute()\n",
    "\n",
    "        # Extract the static coordinates (lat/lon are independent of time)\n",
    "        awi_latitudes = ds_12_months['lat'].values\n",
    "        awi_longitudes = ds_12_months['lon'].values\n",
    "        \n",
    "        # Extract the temperature data (tos) for all 12 months\n",
    "        awi_tos_data_all = ds_12_months['tos']\n",
    "        \n",
    "        # Determine the global min/max for a single, consistent color scale\n",
    "        global_vmin = awi_tos_data_all.min().item()\n",
    "        global_vmax = awi_tos_data_all.max().item()\n",
    "\n",
    "        # --- Visualization (3x4 Multi-Plot Grid) ---\n",
    "        \n",
    "        # Create a figure with 3 rows and 4 columns, sharing the coordinate axes\n",
    "        fig, axes = plt.subplots(\n",
    "            nrows=3, \n",
    "            ncols=4, \n",
    "            figsize=(20, 12), # Increased size for readability\n",
    "            sharex=True, \n",
    "            sharey=True\n",
    "        )\n",
    "        # Flatten the 3x4 array of axes for easy iteration\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        fig.suptitle(f\"AWI-CM-1-1-MR Unstructured Grid: TOS Across 12 Months\", fontsize=20, y=0.95)\n",
    "\n",
    "        # Loop through all 12 time steps (months)\n",
    "        for i, ax in enumerate(axes):\n",
    "            \n",
    "            # Select the data for the current month\n",
    "            tos_data_month = awi_tos_data_all.isel(time=i).values\n",
    "            month_label = ds_12_months.time.dt.strftime('%Y-%m').isel(time=i).item()\n",
    "            \n",
    "            # Plot the unstructured data for this month\n",
    "            scatter = ax.scatter(\n",
    "                awi_longitudes, \n",
    "                awi_latitudes, \n",
    "                s=1, # Small size for performance\n",
    "                c=tos_data_month, \n",
    "                cmap='coolwarm', \n",
    "                vmin=global_vmin, # Use global bounds\n",
    "                vmax=global_vmax\n",
    "            )\n",
    "            \n",
    "            # Set the title for the subplot\n",
    "            ax.set_title(month_label, fontsize=12)\n",
    "            ax.set_aspect('equal', adjustable='box') # Keep aspect ratio for spatial data\n",
    "            ax.tick_params(labelsize=8) # Smaller ticks for subplots\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Label the outer rows/columns\n",
    "            if i % 4 == 0:\n",
    "                ax.set_ylabel(\"Latitude\")\n",
    "            if i >= 8:\n",
    "                ax.set_xlabel(\"Longitude\")\n",
    "\n",
    "\n",
    "        # --- Add a Single Colorbar for the Entire Figure ---\n",
    "        # The colorbar is created using the last scatter object and placed in an external axis\n",
    "        # (This is a common trick for shared colorbars in grids)\n",
    "        cbar_ax = fig.add_axes([0.92, 0.1, 0.02, 0.75]) # [left, bottom, width, height]\n",
    "        fig.colorbar(scatter, cax=cbar_ax, label=\"Sea Surface Temperature (TOS)\")\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0, 0.9, 1]) # Adjust layout to make room for the colorbar\n",
    "        plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during processing: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50eb30dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting memory intensive operation: Opening and building Dask graph...\n",
      "\n",
      "Memory Tracking Results (MiB):\n",
      "Peak Memory Usage (MiB): 37528.29\n",
      "Peak Memory Usage (GiB): 36.65\n"
     ]
    }
   ],
   "source": [
    "# CELL 2: Open Datasets (Memory Inspection Point 1)\n",
    "\n",
    "# This step loads all metadata and builds the Dask graph. Memory usage spikes here.\n",
    "print(\"Starting memory intensive operation: Opening and building Dask graph...\")\n",
    "\n",
    "# Use a memory profiler wrapper to track peak memory consumption\n",
    "def load_and_concat(file_list):\n",
    "    datasets = [xr.open_dataset(FILE_PATH / f) for f in file_list]\n",
    "    # The moment xr.concat is called, the full Dask graph is built\n",
    "    combined_ds = xr.concat(datasets, dim='time', data_vars='all').sortby('time')\n",
    "    return combined_ds\n",
    "\n",
    "# Run the task and track memory (measured in MiB by memory_usage)\n",
    "# If the previous attempts failed at 20G, the peak_mem here should exceed 20480 MiB.\n",
    "peak_mem, combined_ds = memory_usage((load_and_concat, (files_to_process,)), \n",
    "                                       interval=0.1, max_usage=True, retval=True)\n",
    "\n",
    "print(f\"\\nMemory Tracking Results (MiB):\")\n",
    "print(f\"Peak Memory Usage (MiB): {peak_mem:.2f}\")\n",
    "print(f\"Peak Memory Usage (GiB): {(peak_mem / 1024):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ec7920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 3: Final Write (Memory Inspection Point 2)\n",
    "\n",
    "# Write the data with compression. This forces computation of the Dask graph.\n",
    "encoding = {\n",
    "    var: {'zlib': True, 'complevel': 7} \n",
    "    for var in combined_ds.data_vars\n",
    "}\n",
    "\n",
    "print(\"Starting disk write with compression...\")\n",
    "start_time = time.time()\n",
    "combined_ds.to_netcdf(OUTPUT_FILE, encoding=encoding, engine='netcdf4')\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Write successful: {OUTPUT_FILE}\")\n",
    "print(f\"Time taken: {(end_time - start_time):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7822b2af",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'idd_climate_models.constants' has no attribute 'DATA_SOURCE'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# --- Mock Data to Simulate AWI Model ---\u001b[39;00m\n\u001b[32m     17\u001b[39m MODEL_NAME = \u001b[33m\"\u001b[39m\u001b[33mAWI-CM-1-1-MR\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m SOURCE_PATH = \u001b[38;5;28mstr\u001b[39m(rfc.PROCESSED_DATA_PATH / \u001b[43mrfc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDATA_SOURCE\u001b[49m) \u001b[38;5;66;03m# e.g., /.../data/processed/cmip6\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# --- Specific AWI file path (used only for reference) ---\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# The validation system will internally look at this path structure:\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# SOURCE_PATH/MODEL_NAME/.../frequency_folder\u001b[39;00m\n\u001b[32m     23\u001b[39m DUMMY_DATA_TYPE = \u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAttributeError\u001b[39m: module 'idd_climate_models.constants' has no attribute 'DATA_SOURCE'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Imports needed for the test ---\n",
    "# NOTE: These imports rely on your custom package structure\n",
    "try:\n",
    "    import idd_climate_models.constants as rfc\n",
    "    # These functions must exist in your current validation_functions.py\n",
    "    from idd_climate_models.validation_functions import validate_model_in_source\n",
    "except ImportError as e:\n",
    "    print(\"FATAL: Cannot import project modules. Ensure python path is correct.\")\n",
    "    print(f\"Error: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# --- Mock Data to Simulate AWI Model ---\n",
    "MODEL_NAME = \"AWI-CM-1-1-MR\"\n",
    "SOURCE_PATH = str(rfc.PROCESSED_DATA_PATH / rfc.DATA_SOURCE) # e.g., /.../data/processed/cmip6\n",
    "\n",
    "# --- Specific AWI file path (used only for reference) ---\n",
    "# The validation system will internally look at this path structure:\n",
    "# SOURCE_PATH/MODEL_NAME/.../frequency_folder\n",
    "DUMMY_DATA_TYPE = \"data\"\n",
    "DUMMY_DATA_SOURCE = \"cmip6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5c1d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# =========================================================================\n",
    "# STEP 1: Execute the single-model validation with the strict flag ON\n",
    "# =========================================================================\n",
    "\n",
    "print(f\"Starting isolated validation check for model: {MODEL_NAME}\")\n",
    "print(f\"Source path: {SOURCE_PATH}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# We force the strict_grid_check=True flag here.\n",
    "result = validate_model_in_source(\n",
    "    model_name=MODEL_NAME,\n",
    "    source_path=SOURCE_PATH,\n",
    "    data_type=DUMMY_DATA_TYPE,\n",
    "    data_source=DUMMY_DATA_SOURCE,\n",
    "    strict_grid_check=True # <--- CRITICAL FLAG IS ON\n",
    ")\n",
    "\n",
    "# =========================================================================\n",
    "# STEP 2: Analyze the result\n",
    "# =========================================================================\n",
    "\n",
    "print(\"\\n--- Validation Result Summary ---\")\n",
    "print(f\"Model Complete Status: {result.get('complete', 'N/A')}\")\n",
    "print(f\"Top-level Issues ({len(result.get('issues', []))}): {result.get('issues', [])}\")\n",
    "\n",
    "# Look deep into the nested results to find the specific failure reason\n",
    "fail_count = 0\n",
    "for variant, v_data in result.get('variant', {}).items():\n",
    "    for scenario, s_data in v_data.get('scenario', {}).items():\n",
    "        # Check all children (variables/frequencies) for issues\n",
    "        for child_name, child_data in s_data.items():\n",
    "            if child_name == 'variable': # Variable layer\n",
    "                for var, var_data in child_data.items():\n",
    "                    for grid, grid_data in var_data.get('grid', {}).items():\n",
    "                        for freq, freq_data in grid_data.get('frequency', {}).items():\n",
    "                            if not freq_data['complete']:\n",
    "                                fail_count += 1\n",
    "                                print(f\"  FAILURE at {variant}/{scenario}/{var}/{freq}: {freq_data.get('issues', ['No Issues Found?'])}\")\n",
    "\n",
    "if fail_count > 0:\n",
    "    print(f\"\\n❌ FAILED: Found {fail_count} failing sub-runs for {MODEL_NAME}.\")\n",
    "else:\n",
    "    print(\"\\n✅ SUCCESS: Model structure passed the strict grid check.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idd-climate-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
