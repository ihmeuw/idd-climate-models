{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d8b88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation complete for: data, processed, cmip6\n",
      "Summary: 22/23 models complete. Parsed log (up to 'grid') written to /mnt/team/rapidresponse/pub/tropical-storms/data/processed/cmip6/validation_log.json\n",
      "\n",
      "Validation complete for: tc_risk, input, cmip6\n",
      "Summary: 9/22 models complete. Parsed log (up to 'scenario') written to /mnt/team/rapidresponse/pub/tropical-storms/tc_risk/input/cmip6/validation_log.json\n",
      "\n",
      "================================================================================\n",
      "STEP 3: Creating Jobmon tasks (Dynamic Resources)\n",
      "================================================================================\n",
      "Variable: ua | Model: ACCESS-CM2 | Variant: r1i1p1f1 | Scenario: historical | Time Bin: 1970-1989 ->\n",
      "        Requesting Mem: 13G, Run: 15m, Cores: 2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unexpected kwarg. Task Template requires {'data_source', 'model', 'frequency', 'variable', 'grid', 'time_bin', 'variant', 'scenario'}, got {'data_source', 'model', 'frequency', 'variable', 'grid', 'time_bin', 'variant', 'scenario', 'needs_regridding'}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 225\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m        Requesting Mem: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_request[\u001b[33m'\u001b[39m\u001b[33mmemory\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Run: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_request[\u001b[33m'\u001b[39m\u001b[33mruntime\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Cores: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_request[\u001b[33m'\u001b[39m\u001b[33mcores\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    224\u001b[39m \u001b[38;5;66;03m# --- TASK CREATION ---\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m process_task = \u001b[43mprocess_template\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_task\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Inject dynamic resources here\u001b[39;49;00m\n\u001b[32m    227\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompute_resources\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcores\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcores\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mruntime\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mruntime\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mqueue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Use the default queue if specific queue not needed\u001b[39;49;00m\n\u001b[32m    232\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mproject\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_source\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mDATA_SOURCE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariant_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscenario\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mscenario\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtime_bin\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_bin_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvariable\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgrid\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrequency\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfrequency\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m    \u001b[49m\u001b[43mneeds_regridding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mneeds_regridding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m process_tasks.append(process_task)\n\u001b[32m    247\u001b[39m \u001b[38;5;66;03m# Add dependency: process_task depends on folder_task\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/idd-climate-models/lib/python3.12/site-packages/jobmon/client/task_template.py:592\u001b[39m, in \u001b[36mTaskTemplate.create_task\u001b[39m\u001b[34m(self, name, upstream_tasks, task_attributes, max_attempts, compute_resources, compute_resources_callable, resource_scales, cluster_name, fallback_queues, **kwargs)\u001b[39m\n\u001b[32m    590\u001b[39m \u001b[38;5;66;03m# kwargs quality assurance\u001b[39;00m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.active_task_template_version.template_args != \u001b[38;5;28mset\u001b[39m(kwargs.keys()):\n\u001b[32m--> \u001b[39m\u001b[32m592\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    593\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnexpected kwarg. Task Template requires \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    594\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.active_task_template_version.template_args\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mset\u001b[39m(kwargs.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    595\u001b[39m     )\n\u001b[32m    597\u001b[39m \u001b[38;5;66;03m# validate non-empty resource_scale dicts\u001b[39;00m\n\u001b[32m    598\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m resource_scales:\n",
      "\u001b[31mValueError\u001b[39m: Unexpected kwarg. Task Template requires {'data_source', 'model', 'frequency', 'variable', 'grid', 'time_bin', 'variant', 'scenario'}, got {'data_source', 'model', 'frequency', 'variable', 'grid', 'time_bin', 'variant', 'scenario', 'needs_regridding'}"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import sys\n",
    "import getpass\n",
    "import uuid\n",
    "import os\n",
    "from jobmon.client.tool import Tool \n",
    "from pathlib import Path\n",
    "\n",
    "# NOTE: These imports rely on external module definitions (constants, io_compare_utils, etc.)\n",
    "import idd_climate_models.constants as rfc\n",
    "from idd_climate_models.io_compare_utils import compare_model_validation\n",
    "from idd_climate_models.dictionary_utils import parse_results\n",
    "from idd_climate_models.resource_functions import get_rep_file_size_gb, get_resource_info\n",
    "\n",
    "# --- CONSTANT DEFINITIONS (from rfc) ---\n",
    "repo_name = rfc.repo_name\n",
    "package_name = rfc.package_name\n",
    "DATA_DIR = rfc.RAW_DATA_PATH\n",
    "PROCESSED_DATA_PATH = rfc.PROCESSED_DATA_PATH\n",
    "TC_RISK_INPUT_PATH = rfc.TC_RISK_INPUT_PATH\n",
    "SCRIPT_ROOT = rfc.REPO_ROOT / repo_name / \"src\" / package_name / \"01_run_tc_risk\"\n",
    "\n",
    "# Configuration\n",
    "DATA_SOURCE = \"cmip6\"\n",
    "BIN_SIZE_YEARS = 20\n",
    "DRY_RUN = False # Assuming DRY_RUN is generally False for submission\n",
    "RERUN = False\n",
    "\n",
    "\n",
    "INPUT_DATA_TYPE = \"data\"\n",
    "INPUT_IO_TYPE = \"processed\"\n",
    "OUTPUT_DATA_TYPE = \"tc_risk\"\n",
    "OUTPUT_IO_TYPE = \"input\"\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# DATA SETUP & VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "# Use the unified function for validation and comparison\n",
    "validation_info = compare_model_validation(\n",
    "    input_data_type=INPUT_DATA_TYPE,\n",
    "    input_io_type=INPUT_IO_TYPE,\n",
    "    output_data_type=OUTPUT_DATA_TYPE,\n",
    "    output_io_type=OUTPUT_IO_TYPE,\n",
    "    data_source=DATA_SOURCE,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "models_to_process = validation_info[\"models_to_process\"]\n",
    "model_variants_to_run = parse_results(validation_info[\"models_to_process_dict\"], 'variant')\n",
    "\n",
    "# Get the full hierarchy list to build the variable detail map\n",
    "full_path_list = parse_results(validation_info[\"models_to_process_dict\"], 'all')\n",
    "variable_detail_map = {}\n",
    "\n",
    "# Build the map: {(model, variant, scenario, variable): {'grid': 'gn', 'frequency': 'day'}}\n",
    "for item in full_path_list:\n",
    "    key = (\n",
    "        item['model'],\n",
    "        item['variant'],\n",
    "        item['scenario'],\n",
    "        item['variable']\n",
    "    )\n",
    "    # Store the unique grid and frequency needed to build the source_dir\n",
    "    variable_detail_map[key] = {\n",
    "        'grid': item['grid'],\n",
    "        'frequency': item['frequency']\n",
    "    }\n",
    "\n",
    "TIME_BINS = {\n",
    "    scenario: rfc.get_time_bins(scenario, BIN_SIZE_YEARS)\n",
    "    for scenario in rfc.SCENARIOS\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# JOBMON SETUP\n",
    "# ============================================================================\n",
    "\n",
    "user = getpass.getuser()\n",
    "\n",
    "log_dir = Path(\"/mnt/team/idd/pub/\")\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "stdout_dir = log_dir / \"stdout\"\n",
    "stderr_dir = log_dir / \"stderr\" \n",
    "stdout_dir.mkdir(parents=True, exist_ok=True)\n",
    "stderr_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "project = \"proj_rapidresponse\"\n",
    "queue = 'all.q' # Switched to 'all.q' for general use, but kept long.q settings in template\n",
    "\n",
    "wf_uuid = uuid.uuid4()\n",
    "tool_name = f\"{package_name}_tc_risk_reorganization_tool\"\n",
    "tool = Tool(name=tool_name)\n",
    "\n",
    "workflow = tool.create_workflow(\n",
    "    name=f\"{tool_name}_workflow_{wf_uuid}\",\n",
    "    max_concurrently_running=5000,\n",
    ")\n",
    "\n",
    "# Default compute resources for small tasks (Folder Creation)\n",
    "workflow.set_default_compute_resources_from_dict(\n",
    "    cluster_name=\"slurm\",\n",
    "    dictionary={\n",
    "        \"memory\": \"5G\",\n",
    "        \"cores\": 1,\n",
    "        \"runtime\": \"10m\",\n",
    "        \"queue\": queue,\n",
    "        \"project\": project,\n",
    "    }\n",
    ")\n",
    "\n",
    "# LEVEL 1: Folder creation task template\n",
    "folder_template = tool.get_task_template(\n",
    "    template_name=\"create_folders\",\n",
    "    default_cluster_name=\"slurm\",\n",
    "    default_compute_resources={\n",
    "        \"memory\": \"1G\",\n",
    "        \"cores\": 1,\n",
    "        \"runtime\": \"5m\",\n",
    "        \"queue\": queue,\n",
    "        \"project\": project,\n",
    "    },\n",
    "    command_template=(\n",
    "        \"python {script_root}/1a_create_tc_risk_input_folder.py \"\n",
    "        \"--data_source {{data_source}} \" \n",
    "        \"--model {{model}} \"\n",
    "        \"--variant {{variant}} \" \n",
    "        \"--scenario {{scenario}} \"\n",
    "        \"--time_bin {{time_bin}} \"\n",
    "    ).format(script_root=SCRIPT_ROOT),\n",
    "    node_args=[\"data_source\", \"model\", \"variant\", \"scenario\", \"time_bin\"],\n",
    "    task_args=[],\n",
    "    op_args=[],\n",
    ")\n",
    "\n",
    "# LEVEL 2: Processing task template (Dynamic Resources must be applied here)\n",
    "process_template = tool.get_task_template(\n",
    "    template_name=\"process_variable_frequency\",\n",
    "    default_cluster_name=\"slurm\",\n",
    "    default_compute_resources={\n",
    "        \"memory\": \"15G\",\n",
    "        \"cores\": 4, # Changed from 1 to match expected need for xarray ops\n",
    "        \"runtime\": \"1h\",\n",
    "        \"queue\": 'long.q', # Default to a longer queue for processing\n",
    "        \"project\": project,\n",
    "    },\n",
    "    command_template=(\n",
    "        \"python {script_root}/1b_process_time_chunk.py \"\n",
    "        \"--data_source {{data_source}} \"\n",
    "        \"--model {{model}} \"\n",
    "        \"--variant {{variant}} \"\n",
    "        \"--scenario {{scenario}} \"\n",
    "        \"--time_bin {{time_bin}} \"\n",
    "        \"--variable {{variable}} \"\n",
    "        \"--grid {{grid}} \"\n",
    "        \"--frequency {{frequency}} \"\n",
    "        \"--needs_regridding {{needs_regridding}} \"\n",
    "    ).format(script_root=SCRIPT_ROOT),\n",
    "    node_args=[\"data_source\", \"model\", \"variant\", \"scenario\", \"time_bin\", \"variable\", \"grid\", \"frequency\", \"needs_regridding\"],\n",
    "    task_args=[],\n",
    "    op_args=[],\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# TASK CREATION AND DEPENDENCY SETUP (Single Loop)\n",
    "# ============================================================================\n",
    "\n",
    "folder_task_map = {}\n",
    "folder_tasks = []\n",
    "process_tasks = []\n",
    "dependencies = []\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 3: Creating Jobmon tasks (Dynamic Resources)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for mv_info in model_variants_to_run:\n",
    "    model_name = mv_info['model']\n",
    "    variant_name = mv_info['variant']\n",
    "    \n",
    "    for scenario in rfc.SCENARIOS:\n",
    "        for time_bin_tuple in TIME_BINS[scenario]:\n",
    "            time_bin_str = f\"{time_bin_tuple[0]}-{time_bin_tuple[1]}\"\n",
    "            \n",
    "            parent_key = (model_name, variant_name, scenario, time_bin_str)\n",
    "            \n",
    "            # --- LEVEL 1: CREATE PARENT (FOLDER) TASK ---\n",
    "            folder_task = folder_template.create_task(\n",
    "                data_source = DATA_SOURCE,\n",
    "                model = model_name,\n",
    "                variant = variant_name,\n",
    "                scenario = scenario,\n",
    "                time_bin = time_bin_str,\n",
    "            )\n",
    "            \n",
    "            folder_tasks.append(folder_task)\n",
    "            folder_task_map[parent_key] = folder_task\n",
    "            \n",
    "            # --- LEVEL 2: CREATE CHILD (PROCESSING) TASKS AND DEPENDENCIES ---\n",
    "            for variable in rfc.VARIABLES[DATA_SOURCE]:\n",
    "                \n",
    "                detail_lookup_key = (model_name, variant_name, scenario, variable)\n",
    "                details = variable_detail_map.get(detail_lookup_key)\n",
    "                \n",
    "                if not details:\n",
    "                    continue \n",
    "\n",
    "                # --- DYNAMIC RESOURCE CALCULATION ---\n",
    "                \n",
    "                # 1. CONSTRUCT PATH TO A REPRESENTATIVE INPUT FILE\n",
    "                source_file_dir = PROCESSED_DATA_PATH / DATA_SOURCE / model_name / variant_name / scenario / variable / details['grid'] / details['frequency']\n",
    "                resource_request, needs_regridding = get_resource_info(file_path=source_file_dir, representative='first', num_files = BIN_SIZE_YEARS)\n",
    "                print(f\"Variable: {variable} | Model: {model_name} | Variant: {variant_name} | Scenario: {scenario} | Time Bin: {time_bin_str} ->\" )\n",
    "                print(f\"        Requesting Mem: {resource_request['memory']}, Run: {resource_request['runtime']}, Cores: {resource_request['cores']}\")\n",
    "                # --- TASK CREATION ---\n",
    "                process_task = process_template.create_task(\n",
    "                    # Inject dynamic resources here\n",
    "                    compute_resources={\n",
    "                        \"memory\": resource_request[\"memory\"],\n",
    "                        \"cores\": resource_request[\"cores\"],\n",
    "                        \"runtime\": resource_request[\"runtime\"],\n",
    "                        \"queue\": queue, # Use the default queue if specific queue not needed\n",
    "                        \"project\": project,\n",
    "                    },\n",
    "                    data_source = DATA_SOURCE,\n",
    "                    model = model_name,\n",
    "                    variant = variant_name,\n",
    "                    scenario = scenario,\n",
    "                    time_bin = time_bin_str,\n",
    "                    variable = variable,\n",
    "                    grid = details['grid'],\n",
    "                    frequency = details['frequency'],\n",
    "                    needs_regridding=needs_regridding,\n",
    "                )\n",
    "                \n",
    "                process_tasks.append(process_task)\n",
    "                \n",
    "                # Add dependency: process_task depends on folder_task\n",
    "                dependencies.append((process_task, folder_task))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7ba6dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ANALYZING FAILURES FOR INCORRECTLY FILTERED MODELS\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "ANALYZING FAILURES (Highest-Level Issue Per Incomplete Model)\n",
      "================================================================================\n",
      "✗ INCOMPLETE: Validation Failed at: **Model=AWI-CM-1-1-MR -> variant=r1i1p1f1 -> scenario=historical -> variable=tos -> grid=gn -> frequency=Omon**\n",
      "Issues (1): Forbidden unstructured grid dimensions found: ['ncells']. Model is incompatible with the target TC risk grid.\n",
      "================================================================================\n",
      "Model: AWI-CM-1-1-MR -> ✗ INCOMPLETE: Validation Failed at: **Model=AWI-CM-1-1-MR -> variant=r1i1p1f1 -> scenario=historical -> variable=tos -> grid=gn -> frequency=Omon**\n",
      "Issues (1): Forbidden unstructured grid dimensions found: ['ncells']. Model is incompatible with the target TC risk grid.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Assuming validation_info exists from the pipeline run\n",
    "from idd_climate_models.dictionary_utils import summarize_all_failures\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ANALYZING FAILURES FOR INCORRECTLY FILTERED MODELS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Summarize the failures from the input validation dictionary\n",
    "failure_summary = summarize_all_failures(validation_info['input_validation_dict'])\n",
    "\n",
    "# Print the specific issues\n",
    "for model, summary in failure_summary.items():\n",
    "    print(f\"Model: {model} -> {summary}\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "663f00b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'complete': False,\n",
       " 'files': [{'path': '/mnt/team/rapidresponse/pub/tropical-storms/tc_risk/input/cmip6/AWI-CM-1-1-MR/r1i1p1f1/ssp245/2095-2100/hus_Amon_AWI-CM-1-1-MR_ssp245_r1i1p1f1_gn_209501-210012.nc'},\n",
       "  {'path': '/mnt/team/rapidresponse/pub/tropical-storms/tc_risk/input/cmip6/AWI-CM-1-1-MR/r1i1p1f1/ssp245/2095-2100/psl_Amon_AWI-CM-1-1-MR_ssp245_r1i1p1f1_gn_209501-210012.nc'},\n",
       "  {'path': '/mnt/team/rapidresponse/pub/tropical-storms/tc_risk/input/cmip6/AWI-CM-1-1-MR/r1i1p1f1/ssp245/2095-2100/ta_Amon_AWI-CM-1-1-MR_ssp245_r1i1p1f1_gn_209501-210012.nc'},\n",
       "  {'path': '/mnt/team/rapidresponse/pub/tropical-storms/tc_risk/input/cmip6/AWI-CM-1-1-MR/r1i1p1f1/ssp245/2095-2100/ua_day_AWI-CM-1-1-MR_ssp245_r1i1p1f1_gn_20950101-21001231.nc'},\n",
       "  {'path': '/mnt/team/rapidresponse/pub/tropical-storms/tc_risk/input/cmip6/AWI-CM-1-1-MR/r1i1p1f1/ssp245/2095-2100/va_day_AWI-CM-1-1-MR_ssp245_r1i1p1f1_gn_20950101-21001231.nc'}],\n",
       " 'issues': [\"Missing required variable files: ['tos']\"]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_info['output_validation_dict']['validation_results']['AWI-CM-1-1-MR']['variant']['r1i1p1f1']['scenario']['ssp245']['time-period']['2095-2100']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "681b8409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files found in source: 54\n",
      "Files selected for 5-year bin (tos Omon): 5\n",
      "Single file size: 0.158 GiB\n"
     ]
    }
   ],
   "source": [
    "# CELL 1: Setup and Configuration (Modified to select 5 files)\n",
    "import xarray as xr\n",
    "import os\n",
    "from pathlib import Path\n",
    "from memory_profiler import memory_usage\n",
    "import time\n",
    "\n",
    "# --- Path to the Directory Containing the Yearly Files ---\n",
    "FILE_PATH = Path(\"/mnt/team/rapidresponse/pub/tropical-storms/data/processed/cmip6/AWI-CM-1-1-MR/r1i1p1f1/historical/tos/gn/Omon\")\n",
    "OUTPUT_FILE = Path(\"./temp_tos_combined_test.nc\")\n",
    "\n",
    "# We select the first 5 yearly files for the test (simulating a 5-year bin)\n",
    "file_list = sorted([f for f in os.listdir(FILE_PATH) if f.endswith('.nc')])\n",
    "files_to_process = file_list[:5] # <-- Changed to 5 files (1 per year)\n",
    "\n",
    "print(f\"Total files found in source: {len(file_list)}\")\n",
    "print(f\"Files selected for 5-year bin (tos Omon): {len(files_to_process)}\")\n",
    "\n",
    "# Test the size of a single file (should be ~169MB)\n",
    "if files_to_process:\n",
    "    single_file_size_gb = os.path.getsize(FILE_PATH / files_to_process[0]) / (1024**3)\n",
    "    print(f\"Single file size: {single_file_size_gb:.3f} GiB\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"No files found to process.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efd96542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting file: tos_Omon_ACCESS-CM2_historical_r1i1p1f1_gn_195001-195012.nc\n",
      "\n",
      "--- Dataset Dimensions & Variables for ACCESS-CM2 ---\n",
      "<xarray.Dataset> Size: 14MB\n",
      "Dimensions:             (time: 12, bnds: 2, j: 300, i: 360, vertices: 4)\n",
      "Coordinates:\n",
      "  * time                (time) datetime64[ns] 96B 1950-01-16T12:00:00 ... 195...\n",
      "  * j                   (j) int32 1kB 0 1 2 3 4 5 6 ... 294 295 296 297 298 299\n",
      "  * i                   (i) int32 1kB 0 1 2 3 4 5 6 ... 354 355 356 357 358 359\n",
      "    latitude            (j, i) float64 864kB ...\n",
      "    longitude           (j, i) float64 864kB ...\n",
      "Dimensions without coordinates: bnds, vertices\n",
      "Data variables:\n",
      "    time_bnds           (time, bnds) datetime64[ns] 192B ...\n",
      "    vertices_latitude   (j, i, vertices) float64 3MB ...\n",
      "    vertices_longitude  (j, i, vertices) float64 3MB ...\n",
      "    tos                 (time, j, i) float32 5MB ...\n",
      "Attributes: (12/47)\n",
      "    Conventions:            CF-1.7 CMIP-6.2\n",
      "    activity_id:            CMIP\n",
      "    branch_method:          standard\n",
      "    branch_time_in_child:   0.0\n",
      "    branch_time_in_parent:  0.0\n",
      "    creation_date:          2019-11-08T18:45:44Z\n",
      "    ...                     ...\n",
      "    variable_id:            tos\n",
      "    variant_label:          r1i1p1f1\n",
      "    version:                v20191108\n",
      "    cmor_version:           3.4.0\n",
      "    tracking_id:            hdl:21.14100/0bcaaa74-aedb-4d45-a5e5-cb3ab467f2b5\n",
      "    license:                CMIP6 model data produced by CSIRO is licensed un...\n",
      "\n",
      "--- Coordinates and Their Shapes ---\n",
      "Coordinate: time, Shape: (12,), Dims: ('time',)\n",
      "Coordinate: j, Shape: (300,), Dims: ('j',)\n",
      "Coordinate: i, Shape: (360,), Dims: ('i',)\n",
      "Coordinate: latitude, Shape: (300, 360), Dims: ('j', 'i')\n",
      "Coordinate: longitude, Shape: (300, 360), Dims: ('j', 'i')\n",
      "\n",
      "--- Check Memory Estimate of Single File ---\n",
      "Total estimated memory (raw data): 0.013 GiB\n"
     ]
    }
   ],
   "source": [
    "# CELL 1: Load a Single File and Inspect Metadata\n",
    "\n",
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# --- Path to the ACCESS-CM2 Directory ---\n",
    "FILE_PATH = Path(\"/mnt/team/rapidresponse/pub/tropical-storms/data/processed/cmip6/ACCESS-CM2/r1i1p1f1/historical/tos/gn/Omon\")\n",
    "file_list = sorted([f for f in os.listdir(FILE_PATH) if f.endswith('.nc')])\n",
    "first_file = FILE_PATH / file_list[0]\n",
    "\n",
    "print(f\"Inspecting file: {first_file.name}\")\n",
    "\n",
    "# 1. Open a single dataset lazily\n",
    "ds = xr.open_dataset(first_file)\n",
    "\n",
    "print(f\"\\n--- Dataset Dimensions & Variables for ACCESS-CM2 ---\")\n",
    "print(ds)\n",
    "\n",
    "print(f\"\\n--- Coordinates and Their Shapes ---\")\n",
    "# Check the shape of the lat/lon arrays. If they are simple 1D arrays, memory overhead is low.\n",
    "for name, data_array in ds.coords.items():\n",
    "    print(f\"Coordinate: {name}, Shape: {data_array.shape}, Dims: {data_array.dims}\")\n",
    "\n",
    "print(f\"\\n--- Check Memory Estimate of Single File ---\")\n",
    "print(f\"Total estimated memory (raw data): {ds.nbytes / (1024**3):.3f} GiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e598f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Dataset Dimensions & Variables ---\n",
      "<xarray.Dataset> Size: 624MB\n",
      "Dimensions:    (time: 120, bnds: 2, ncells: 830305, vertices: 16)\n",
      "Coordinates:\n",
      "  * time       (time) datetime64[ns] 960B 1961-01-16T12:00:00 ... 1970-12-16T...\n",
      "    lat        (ncells) float64 7MB ...\n",
      "    lon        (ncells) float64 7MB ...\n",
      "Dimensions without coordinates: bnds, ncells, vertices\n",
      "Data variables:\n",
      "    time_bnds  (time, bnds) datetime64[ns] 2kB ...\n",
      "    tos        (time, ncells) float32 399MB ...\n",
      "    lat_bnds   (ncells, vertices) float64 106MB ...\n",
      "    lon_bnds   (ncells, vertices) float64 106MB ...\n",
      "Attributes: (12/39)\n",
      "    frequency:              mon\n",
      "    activity_id:            CMIP\n",
      "    Conventions:            CF-1.7 CMIP-6.2\n",
      "    creation_date:          2018-12-18T12:00:00Z\n",
      "    data_specs_version:     01.00.27\n",
      "    experiment:             historical\n",
      "    ...                     ...\n",
      "    parent_activity_id:     CMIP\n",
      "    parent_experiment_id:   piControl\n",
      "    parent_mip_era:         CMIP6\n",
      "    parent_source_id:       AWI-CM-1-1-MR\n",
      "    parent_time_units:      days since 2401-1-1\n",
      "    parent_variant_label:   r1i1p1f1\n",
      "\n",
      "--- Coordinates and Their Shapes ---\n",
      "Coordinate: time, Shape: (120,), Dims: ('time',)\n",
      "Coordinate: lat, Shape: (830305,), Dims: ('ncells',)\n",
      "Coordinate: lon, Shape: (830305,), Dims: ('ncells',)\n",
      "\n",
      "--- Check Memory Estimate of Single File ---\n",
      "Total estimated memory (raw data): 0.582 GiB\n"
     ]
    }
   ],
   "source": [
    "# CELL 1: Load a Single File and Inspect Metadata\n",
    "\n",
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "\n",
    "FILE_PATH = Path(\"/mnt/team/rapidresponse/pub/tropical-storms/data/raw/cmip6/AWI-CM-1-1-MR/r1i1p1f1/historical/tos/gn/Omon\")\n",
    "file_list = sorted([f for f in os.listdir(FILE_PATH) if f.endswith('.nc')])\n",
    "first_file = FILE_PATH / file_list[0]\n",
    "\n",
    "# 1. Open a single dataset lazily\n",
    "ds = xr.open_dataset(first_file)\n",
    "\n",
    "print(f\"--- Dataset Dimensions & Variables ---\")\n",
    "print(ds)\n",
    "\n",
    "print(f\"\\n--- Coordinates and Their Shapes ---\")\n",
    "# Identify any coordinates or variables that have a large number of indices or dimensions.\n",
    "# Look for large arrays that are not the main 'tos' data variable.\n",
    "for name, data_array in ds.coords.items():\n",
    "    print(f\"Coordinate: {name}, Shape: {data_array.shape}, Dims: {data_array.dims}\")\n",
    "\n",
    "print(f\"\\n--- Check Memory Estimate of Single File ---\")\n",
    "# This estimates the memory of the actual data, NOT the overhead.\n",
    "# Look for a large discrepancy between the .nbytes estimate and the file size (0.165 GiB).\n",
    "print(f\"Total estimated memory (raw data): {ds.nbytes / (1024**3):.3f} GiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25115344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of grid cells (ncells): 830305\n",
      "Shape of latitude array: (830305,)\n",
      "Shape of longitude array: (830305,)\n",
      "Number of unique latitudes: 830305\n",
      "Number of unique longitudes: 830305\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "FILE_PATH = Path(\"/mnt/team/rapidresponse/pub/tropical-storms/data/processed/cmip6/AWI-CM-1-1-MR/r1i1p1f1/historical/tos/gn/Omon\")\n",
    "file_list = sorted([f for f in os.listdir(FILE_PATH) if f.endswith('.nc')])\n",
    "first_file = FILE_PATH / file_list[0]\n",
    "\n",
    "# 1. Open the dataset lazily\n",
    "ds_awi = xr.open_dataset(first_file)\n",
    "\n",
    "# 2. Extract the coordinate arrays (these are large 1D arrays of size 830,305)\n",
    "awi_latitudes = ds_awi['lat'].values\n",
    "awi_longitudes = ds_awi['lon'].values\n",
    "\n",
    "print(f\"Total number of grid cells (ncells): {len(awi_latitudes)}\")\n",
    "print(f\"Shape of latitude array: {awi_latitudes.shape}\")\n",
    "print(f\"Shape of longitude array: {awi_longitudes.shape}\")\n",
    "print(f\"Number of unique latitudes: {len(set(awi_latitudes))}\")\n",
    "print(f\"Number of unique longitudes: {len(set(awi_longitudes))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79f160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can I plot the lat/lon arrays to visualize their distribution?\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(awi_longitudes, awi_latitudes, s=1)\n",
    "plt.title(\"Scatter Plot of AWI-CM-1-1-MR Grid Points\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e878e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np # Used for finding global min/max\n",
    "\n",
    "# --- Configuration ---\n",
    "# Use the raw data path to get a full file for analysis\n",
    "FILE_PATH = Path(\"/mnt/team/rapidresponse/pub/tropical-storms/data/raw/cmip6/AWI-CM-1-1-MR/r1i1p1f1/historical/tos/gn/Omon\")\n",
    "\n",
    "# --- Processing ---\n",
    "\n",
    "try:\n",
    "    file_list = sorted([f for f in os.listdir(FILE_PATH) if f.endswith('.nc')])\n",
    "    if not file_list:\n",
    "        print(f\"Error: No NetCDF files found in {FILE_PATH}\")\n",
    "        exit()\n",
    "        \n",
    "    first_file = FILE_PATH / file_list[0]\n",
    "    \n",
    "    # Open the dataset, loading the first 12 months (or fewer if the file is shorter)\n",
    "    with xr.open_dataset(first_file) as ds_awi:\n",
    "        # Select the first 12 months for plotting (the first year)\n",
    "        # We must compute() here to pull the coordinates and tos data into memory for plotting\n",
    "        ds_12_months = ds_awi.isel(time=slice(0, 12)).compute()\n",
    "\n",
    "        # Extract the static coordinates (lat/lon are independent of time)\n",
    "        awi_latitudes = ds_12_months['lat'].values\n",
    "        awi_longitudes = ds_12_months['lon'].values\n",
    "        \n",
    "        # Extract the temperature data (tos) for all 12 months\n",
    "        awi_tos_data_all = ds_12_months['tos']\n",
    "        \n",
    "        # Determine the global min/max for a single, consistent color scale\n",
    "        global_vmin = awi_tos_data_all.min().item()\n",
    "        global_vmax = awi_tos_data_all.max().item()\n",
    "\n",
    "        # --- Visualization (3x4 Multi-Plot Grid) ---\n",
    "        \n",
    "        # Create a figure with 3 rows and 4 columns, sharing the coordinate axes\n",
    "        fig, axes = plt.subplots(\n",
    "            nrows=3, \n",
    "            ncols=4, \n",
    "            figsize=(20, 12), # Increased size for readability\n",
    "            sharex=True, \n",
    "            sharey=True\n",
    "        )\n",
    "        # Flatten the 3x4 array of axes for easy iteration\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        fig.suptitle(f\"AWI-CM-1-1-MR Unstructured Grid: TOS Across 12 Months\", fontsize=20, y=0.95)\n",
    "\n",
    "        # Loop through all 12 time steps (months)\n",
    "        for i, ax in enumerate(axes):\n",
    "            \n",
    "            # Select the data for the current month\n",
    "            tos_data_month = awi_tos_data_all.isel(time=i).values\n",
    "            month_label = ds_12_months.time.dt.strftime('%Y-%m').isel(time=i).item()\n",
    "            \n",
    "            # Plot the unstructured data for this month\n",
    "            scatter = ax.scatter(\n",
    "                awi_longitudes, \n",
    "                awi_latitudes, \n",
    "                s=1, # Small size for performance\n",
    "                c=tos_data_month, \n",
    "                cmap='coolwarm', \n",
    "                vmin=global_vmin, # Use global bounds\n",
    "                vmax=global_vmax\n",
    "            )\n",
    "            \n",
    "            # Set the title for the subplot\n",
    "            ax.set_title(month_label, fontsize=12)\n",
    "            ax.set_aspect('equal', adjustable='box') # Keep aspect ratio for spatial data\n",
    "            ax.tick_params(labelsize=8) # Smaller ticks for subplots\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Label the outer rows/columns\n",
    "            if i % 4 == 0:\n",
    "                ax.set_ylabel(\"Latitude\")\n",
    "            if i >= 8:\n",
    "                ax.set_xlabel(\"Longitude\")\n",
    "\n",
    "\n",
    "        # --- Add a Single Colorbar for the Entire Figure ---\n",
    "        # The colorbar is created using the last scatter object and placed in an external axis\n",
    "        # (This is a common trick for shared colorbars in grids)\n",
    "        cbar_ax = fig.add_axes([0.92, 0.1, 0.02, 0.75]) # [left, bottom, width, height]\n",
    "        fig.colorbar(scatter, cax=cbar_ax, label=\"Sea Surface Temperature (TOS)\")\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0, 0.9, 1]) # Adjust layout to make room for the colorbar\n",
    "        plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during processing: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50eb30dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting memory intensive operation: Opening and building Dask graph...\n",
      "\n",
      "Memory Tracking Results (MiB):\n",
      "Peak Memory Usage (MiB): 37528.29\n",
      "Peak Memory Usage (GiB): 36.65\n"
     ]
    }
   ],
   "source": [
    "# CELL 2: Open Datasets (Memory Inspection Point 1)\n",
    "\n",
    "# This step loads all metadata and builds the Dask graph. Memory usage spikes here.\n",
    "print(\"Starting memory intensive operation: Opening and building Dask graph...\")\n",
    "\n",
    "# Use a memory profiler wrapper to track peak memory consumption\n",
    "def load_and_concat(file_list):\n",
    "    datasets = [xr.open_dataset(FILE_PATH / f) for f in file_list]\n",
    "    # The moment xr.concat is called, the full Dask graph is built\n",
    "    combined_ds = xr.concat(datasets, dim='time', data_vars='all').sortby('time')\n",
    "    return combined_ds\n",
    "\n",
    "# Run the task and track memory (measured in MiB by memory_usage)\n",
    "# If the previous attempts failed at 20G, the peak_mem here should exceed 20480 MiB.\n",
    "peak_mem, combined_ds = memory_usage((load_and_concat, (files_to_process,)), \n",
    "                                       interval=0.1, max_usage=True, retval=True)\n",
    "\n",
    "print(f\"\\nMemory Tracking Results (MiB):\")\n",
    "print(f\"Peak Memory Usage (MiB): {peak_mem:.2f}\")\n",
    "print(f\"Peak Memory Usage (GiB): {(peak_mem / 1024):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ec7920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 3: Final Write (Memory Inspection Point 2)\n",
    "\n",
    "# Write the data with compression. This forces computation of the Dask graph.\n",
    "encoding = {\n",
    "    var: {'zlib': True, 'complevel': 7} \n",
    "    for var in combined_ds.data_vars\n",
    "}\n",
    "\n",
    "print(\"Starting disk write with compression...\")\n",
    "start_time = time.time()\n",
    "combined_ds.to_netcdf(OUTPUT_FILE, encoding=encoding, engine='netcdf4')\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Write successful: {OUTPUT_FILE}\")\n",
    "print(f\"Time taken: {(end_time - start_time):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7822b2af",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'idd_climate_models.constants' has no attribute 'DATA_SOURCE'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# --- Mock Data to Simulate AWI Model ---\u001b[39;00m\n\u001b[32m     17\u001b[39m MODEL_NAME = \u001b[33m\"\u001b[39m\u001b[33mAWI-CM-1-1-MR\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m SOURCE_PATH = \u001b[38;5;28mstr\u001b[39m(rfc.PROCESSED_DATA_PATH / \u001b[43mrfc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDATA_SOURCE\u001b[49m) \u001b[38;5;66;03m# e.g., /.../data/processed/cmip6\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# --- Specific AWI file path (used only for reference) ---\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# The validation system will internally look at this path structure:\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# SOURCE_PATH/MODEL_NAME/.../frequency_folder\u001b[39;00m\n\u001b[32m     23\u001b[39m DUMMY_DATA_TYPE = \u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAttributeError\u001b[39m: module 'idd_climate_models.constants' has no attribute 'DATA_SOURCE'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Imports needed for the test ---\n",
    "# NOTE: These imports rely on your custom package structure\n",
    "try:\n",
    "    import idd_climate_models.constants as rfc\n",
    "    # These functions must exist in your current validation_functions.py\n",
    "    from idd_climate_models.validation_functions import validate_model_in_source\n",
    "except ImportError as e:\n",
    "    print(\"FATAL: Cannot import project modules. Ensure python path is correct.\")\n",
    "    print(f\"Error: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# --- Mock Data to Simulate AWI Model ---\n",
    "MODEL_NAME = \"AWI-CM-1-1-MR\"\n",
    "SOURCE_PATH = str(rfc.PROCESSED_DATA_PATH / rfc.DATA_SOURCE) # e.g., /.../data/processed/cmip6\n",
    "\n",
    "# --- Specific AWI file path (used only for reference) ---\n",
    "# The validation system will internally look at this path structure:\n",
    "# SOURCE_PATH/MODEL_NAME/.../frequency_folder\n",
    "DUMMY_DATA_TYPE = \"data\"\n",
    "DUMMY_DATA_SOURCE = \"cmip6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5c1d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# =========================================================================\n",
    "# STEP 1: Execute the single-model validation with the strict flag ON\n",
    "# =========================================================================\n",
    "\n",
    "print(f\"Starting isolated validation check for model: {MODEL_NAME}\")\n",
    "print(f\"Source path: {SOURCE_PATH}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# We force the strict_grid_check=True flag here.\n",
    "result = validate_model_in_source(\n",
    "    model_name=MODEL_NAME,\n",
    "    source_path=SOURCE_PATH,\n",
    "    data_type=DUMMY_DATA_TYPE,\n",
    "    data_source=DUMMY_DATA_SOURCE,\n",
    "    strict_grid_check=True # <--- CRITICAL FLAG IS ON\n",
    ")\n",
    "\n",
    "# =========================================================================\n",
    "# STEP 2: Analyze the result\n",
    "# =========================================================================\n",
    "\n",
    "print(\"\\n--- Validation Result Summary ---\")\n",
    "print(f\"Model Complete Status: {result.get('complete', 'N/A')}\")\n",
    "print(f\"Top-level Issues ({len(result.get('issues', []))}): {result.get('issues', [])}\")\n",
    "\n",
    "# Look deep into the nested results to find the specific failure reason\n",
    "fail_count = 0\n",
    "for variant, v_data in result.get('variant', {}).items():\n",
    "    for scenario, s_data in v_data.get('scenario', {}).items():\n",
    "        # Check all children (variables/frequencies) for issues\n",
    "        for child_name, child_data in s_data.items():\n",
    "            if child_name == 'variable': # Variable layer\n",
    "                for var, var_data in child_data.items():\n",
    "                    for grid, grid_data in var_data.get('grid', {}).items():\n",
    "                        for freq, freq_data in grid_data.get('frequency', {}).items():\n",
    "                            if not freq_data['complete']:\n",
    "                                fail_count += 1\n",
    "                                print(f\"  FAILURE at {variant}/{scenario}/{var}/{freq}: {freq_data.get('issues', ['No Issues Found?'])}\")\n",
    "\n",
    "if fail_count > 0:\n",
    "    print(f\"\\n❌ FAILED: Found {fail_count} failing sub-runs for {MODEL_NAME}.\")\n",
    "else:\n",
    "    print(\"\\n✅ SUCCESS: Model structure passed the strict grid check.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idd-climate-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
