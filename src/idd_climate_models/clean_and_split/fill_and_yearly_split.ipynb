{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dbcf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from datetime import datetime, timedelta\n",
    "import argparse\n",
    "\n",
    "import idd_climate_models.constants as rfc\n",
    "from idd_climate_models.validate_model_functions import int_to_date, is_monthly\n",
    "\n",
    "PROCESSED_DATA_PATH = rfc.PROCESSED_DATA_PATH\n",
    "\n",
    "# Year filtering constants\n",
    "MIN_YEAR = 1950\n",
    "MAX_YEAR = 2100\n",
    "\n",
    "\n",
    "def define_dest_dir(model, variant, scenario, variable, grid, time_period):\n",
    "    \"\"\"Define the destination directory based on model parameters.\"\"\"\n",
    "    dest_dir = os.path.join(PROCESSED_DATA_PATH, model, variant, scenario, variable, grid, time_period)\n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "    return dest_dir\n",
    "\n",
    "\n",
    "def fill_nans_xarray(ds):\n",
    "    \"\"\"Fill NaN values using xarray's built-in methods - much more efficient!\"\"\"\n",
    "    ds_filled = ds.copy()\n",
    "    \n",
    "    # Method 1: Forward fill then backward fill (good for time series)\n",
    "    # ds_filled = ds.ffill(dim='time').bfill(dim='time')\n",
    "    \n",
    "    # Method 2: Interpolate using nearest neighbor (better for spatial data)\n",
    "    for var in ds.data_vars:\n",
    "        if ds[var].isnull().any():\n",
    "            print(f\"    Filling NaNs in variable '{var}'...\")\n",
    "            # Use xarray's interpolate_na with nearest neighbor\n",
    "            ds_filled[var] = ds[var].interpolate_na(\n",
    "                dim='time', \n",
    "                method='nearest', \n",
    "                fill_value='extrapolate'\n",
    "            )\n",
    "            \n",
    "            # If there are still NaNs after time interpolation, try spatial interpolation\n",
    "            if ds_filled[var].isnull().any():\n",
    "                # Get spatial dimensions (usually lat/lon)\n",
    "                spatial_dims = [dim for dim in ds[var].dims if dim not in ['time']]\n",
    "                for dim in spatial_dims:\n",
    "                    if ds_filled[var].isnull().any():\n",
    "                        ds_filled[var] = ds_filled[var].interpolate_na(\n",
    "                            dim=dim, \n",
    "                            method='nearest',\n",
    "                            fill_value='extrapolate'\n",
    "                        )\n",
    "    \n",
    "    return ds_filled\n",
    "\n",
    "\n",
    "def write_yearly_files_optimized(ds, src_file, dest_dir):\n",
    "    \"\"\"Write dataset split into yearly files with better performance and error handling.\"\"\"\n",
    "    # Get the time folder name from the path\n",
    "    time_folder = os.path.basename(os.path.dirname(src_file))\n",
    "    is_monthly = 'mon' in time_folder.lower()\n",
    "    is_daily = time_folder.lower() == 'day'\n",
    "    \n",
    "    # Get all years and filter to specified range\n",
    "    all_years = np.unique(ds[\"time.year\"].values)\n",
    "    years = [year for year in all_years if MIN_YEAR <= year <= MAX_YEAR]\n",
    "    \n",
    "    print(f\"  Time frequency: {'Monthly' if is_monthly else 'Daily' if is_daily else 'Unknown'}\")\n",
    "    \n",
    "    # Report filtering results\n",
    "    if len(years) < len(all_years):\n",
    "        excluded_years = [year for year in all_years if year < MIN_YEAR or year > MAX_YEAR]\n",
    "        print(f\"  Filtering years {MIN_YEAR}-{MAX_YEAR}: keeping {len(years)}/{len(all_years)} years\")\n",
    "        if len(excluded_years) <= 10:  # Only show if not too many\n",
    "            print(f\"  Excluded years: {excluded_years}\")\n",
    "        else:\n",
    "            print(f\"  Excluded {len(excluded_years)} years outside range\")\n",
    "    else:\n",
    "        print(f\"  All {len(years)} years are within {MIN_YEAR}-{MAX_YEAR} range\")\n",
    "    \n",
    "    if len(years) > 0:\n",
    "        print(f\"  Years to process: {len(years)} ({years[0]}-{years[-1]})\")\n",
    "    else:\n",
    "        print(f\"  No years to process (all outside {MIN_YEAR}-{MAX_YEAR} range)\")\n",
    "        return\n",
    "    \n",
    "    # Process filtered years\n",
    "    for year in years:\n",
    "        try:\n",
    "            # More efficient year selection\n",
    "            ds_year = ds.sel(time=ds.time.dt.year == year)\n",
    "            \n",
    "            # Skip empty years\n",
    "            if len(ds_year.time) == 0:\n",
    "                print(f\"    Skipping {year} (no data)\")\n",
    "                continue\n",
    "            \n",
    "            # Generate output filename\n",
    "            base_name = os.path.basename(src_file)\n",
    "            if is_monthly:\n",
    "                out_fname = re.sub(r'_(\\d{6})-(\\d{6})\\.nc$', f'_{year}01-{year}12.nc', base_name)\n",
    "            elif is_daily:\n",
    "                out_fname = re.sub(r'_(\\d{8})-(\\d{8})\\.nc$', f'_{year}0101-{year}1231.nc', base_name)\n",
    "            else:\n",
    "                # Fallback for unknown formats\n",
    "                out_fname = re.sub(r'_(\\d{6,8})-(\\d{6,8})\\.nc$', f'_{year}.nc', base_name)\n",
    "            \n",
    "            out_path = os.path.join(dest_dir, out_fname)\n",
    "            \n",
    "            # Write with optimized settings\n",
    "            encoding = {}\n",
    "            for var in ds_year.data_vars:\n",
    "                encoding[var] = {\n",
    "                    'zlib': True,           # Enable compression\n",
    "                    'complevel': 4,         # Compression level (1-9)\n",
    "                    'shuffle': True,        # Improve compression\n",
    "                    'chunksizes': None      # Let xarray choose optimal chunks\n",
    "                }\n",
    "            \n",
    "            ds_year.to_netcdf(\n",
    "                out_path, \n",
    "                encoding=encoding,\n",
    "                engine='netcdf4'\n",
    "            )\n",
    "            \n",
    "            print(f\"    Wrote: {out_fname} ({len(ds_year.time)} time steps)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    ✗ Failed to write year {year}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "def process_file(file_path, dest_dir):\n",
    "    \"\"\"Process a single NetCDF file: fill NaNs and split into yearly files.\"\"\"\n",
    "    try:\n",
    "        start_time = datetime.now()\n",
    "        print(f\"Processing: {os.path.basename(file_path)}\")\n",
    "        \n",
    "        # Open dataset with chunking for better memory management\n",
    "        ds = xr.open_dataset(file_path, engine='netcdf4', chunks={'time': 100})\n",
    "        \n",
    "        print(f\"  Dataset info: {len(ds.time)} time steps, {list(ds.data_vars.keys())} variables\")\n",
    "        \n",
    "        # Check for NaNs more efficiently\n",
    "        print(\"  Checking for NaNs...\")\n",
    "        nan_vars = []\n",
    "        for var in ds.data_vars:\n",
    "            if ds[var].isnull().any():\n",
    "                nan_count = ds[var].isnull().sum().values\n",
    "                nan_vars.append((var, nan_count))\n",
    "        \n",
    "        has_nans = len(nan_vars) > 0\n",
    "        \n",
    "        if has_nans:\n",
    "            print(f\"  Found NaNs in {len(nan_vars)} variables:\")\n",
    "            for var, count in nan_vars:\n",
    "                print(f\"    {var}: {count} NaN values\")\n",
    "            print(\"  Filling NaNs using interpolation...\")\n",
    "            ds_filled = fill_nans_xarray(ds)\n",
    "        else:\n",
    "            print(\"  No NaNs found.\")\n",
    "            ds_filled = ds\n",
    "        \n",
    "        # Write yearly files\n",
    "        print(\"  Writing yearly files...\")\n",
    "        write_yearly_files_optimized(ds_filled, file_path, dest_dir)\n",
    "        \n",
    "        # Cleanup\n",
    "        ds.close()\n",
    "        if has_nans:\n",
    "            ds_filled.close()\n",
    "        \n",
    "        # Report results\n",
    "        elapsed = (datetime.now() - start_time).total_seconds()\n",
    "        status = f\"filled NaNs in {len(nan_vars)} vars\" if has_nans else \"no NaNs\"\n",
    "        \n",
    "        # Filter years for final report\n",
    "        all_years = np.unique(ds.time.dt.year.values) if hasattr(ds, 'time') else []\n",
    "        filtered_years = [year for year in all_years if MIN_YEAR <= year <= MAX_YEAR]\n",
    "        \n",
    "        print(f\"  ✓ Completed in {elapsed:.1f}s ({status}, {len(filtered_years)} years processed)\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error processing {os.path.basename(file_path)}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to process climate model data.\"\"\"\n",
    "    # Set up argument parser\n",
    "    parser = argparse.ArgumentParser(description=\"Fill and yearly split climate model data\")\n",
    "    parser.add_argument(\"--model\", type=str, required=True, help=\"Climate model name\")\n",
    "    parser.add_argument(\"--variant\", type=str, required=True, help=\"Model variant\")\n",
    "    parser.add_argument(\"--scenario\", type=str, required=True, help=\"Climate scenario\")\n",
    "    parser.add_argument(\"--variable\", type=str, required=True, help=\"Climate variable\")\n",
    "    parser.add_argument(\"--grid\", type=str, required=True, help=\"Grid type\")\n",
    "    parser.add_argument(\"--time_period\", type=str, required=True, help=\"Time period of the data\")\n",
    "    parser.add_argument(\"--file_path\", type=str, required=True, help=\"Path to the input file\")\n",
    "    \n",
    "    # Parse arguments\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Validate input file exists\n",
    "    if not os.path.exists(args.file_path):\n",
    "        print(f\"Error: Input file does not exist: {args.file_path}\")\n",
    "        return 1\n",
    "    \n",
    "    # Create destination directory\n",
    "    dest_dir = define_dest_dir(\n",
    "        args.model, args.variant, args.scenario, \n",
    "        args.variable, args.grid, args.time_period\n",
    "    )\n",
    "    \n",
    "    print(f\"Processing climate model data:\")\n",
    "    print(f\"  Model: {args.model}\")\n",
    "    print(f\"  Variant: {args.variant}\")\n",
    "    print(f\"  Scenario: {args.scenario}\")\n",
    "    print(f\"  Variable: {args.variable}\")\n",
    "    print(f\"  Grid: {args.grid}\")\n",
    "    print(f\"  Time period: {args.time_period}\")\n",
    "    print(f\"  Input file: {args.file_path}\")\n",
    "    print(f\"  Output directory: {dest_dir}\")\n",
    "    print(f\"  Year range: {MIN_YEAR}-{MAX_YEAR}\")\n",
    "    print()\n",
    "    \n",
    "    # Process the file\n",
    "    success = process_file(args.file_path, dest_dir)\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\n✓ Processing completed successfully!\")\n",
    "        return 0\n",
    "    else:\n",
    "        print(\"\\n✗ Processing failed!\")\n",
    "        return 1\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    exit(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idd-climate-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
